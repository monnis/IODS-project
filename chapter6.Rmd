#Chapter 6: Analysis of longitudinal data

**Part 0: Libraries used and overall setup**

```{r echo=TRUE, warning=FALSE} 

rm(list=ls())

library(dplyr); library(ggplot2); library(GGally); library(readr); library(tidyr); library(lme4) 

setwd("~/GitHub/IODS-project/data")

ratsl <- read.table("RATSL.txt")
bprsl <- read.table("BPRSL.txt")

ratsl$ID <- factor(ratsl$ID)
ratsl$Group <- factor(ratsl$Group)

```


###I: Implementing the analyses of chapter 8 using the RATS data 

I am quite slavishly following the chapters in the books.Like a rat following the pied piper. 


The data used in this first part:

*Rats - Three different groups of rats (a total of 16 rats) are fed different food and change in their weight is observed. The data is originally from Crowder & Hand (1990). Used in this first part.


First, I plot the individual rat observations by time. Group signals the colour. 

[*Important notice:* In the Vehkalahti & Everett (2018?) chapter 8, there layout was three graphs, one graph per group that is, but I chose to plot all the rats in the same graph and just distinguish the groups by color, as there are only 16 rats. The information provided by both approaches is somewhat equal.]

Whoa, there is one huge rat in the second group! Otherwise, the initial levels of the rat weight and the slopes seem to be quite close together group-wise.


![](C:\Users\Juha\Desktop\capybara.jpg) The biggest rodent in the world is capybara.

![](C:\Users\Juha\Desktop\capybara.gif) Capybaras are wicked cool animals.

![](C:\Users\Juha\Desktop\capybara2.gif) Respected by all.

[What's not to love.](https://www.smithsonianmag.com/smart-news/capybaras-are-basically-natures-chairs-180949677/)


```{r} 

ggplot(ratsl, aes(x = Time, y = Weight, group=ID, col = Group)) +
  geom_line() +
  scale_linetype_manual(values = rep(1:10, times=4))  + 
  scale_y_continuous(name = "rat weight", limits = c(min(ratsl$Weight), max(ratsl$Weight))) 

```

Now if we standardize the weights (below), the slopes get more equal.There *seems* to be no differences in the growth rates of the rates time-wise. 
 

```{r} 
ratsl <- ratsl %>%
  group_by(Time) %>%
  mutate(stdweight = scale(Weight)) %>%
  ungroup()

ggplot(ratsl, aes(x = Time, y = stdweight, group=ID, col = Group )) +
  geom_line() +
  scale_linetype_manual(values = rep(1:10, times=4)) +
  scale_y_continuous(name = "Standardized rat weight")

```


Next, I plot how the mean weight has changed by group.

Not that much new information here, as anticipated, the standard errors are highest among the second group. Yup, the one with that huge rat.


```{r} 

n <- ratsl$Time %>% unique() %>% length()

ratsass <- ratsl %>%
  group_by(Group, Time) %>%
  summarise( mean = mean(Weight), se = sd(Weight)/sqrt(n)) %>%
  ungroup()

ggplot(ratsass, aes(x = Time, y = mean, linetype = Group, shape = Group)) +
  geom_line() +
  geom_point(size=3) +
  geom_errorbar(aes(ymin=mean-se, ymax=mean+se, linetype="1"), width=0.3) +
  theme(legend.position = c(0.9,0.5)) +
  scale_y_continuous(name = "mean(Weight) +/- se(Weight)")

```



Summary measures approach in the form of boxplots. Boxes by groups. Excluding that one huge rat. The groups differ drastically. 


```{r} 

ratsass2 <- ratsl %>%
  filter(Time > 1) %>%
  group_by(Group, ID) %>%
  summarise( mean=mean(Weight) ) %>%
  ungroup()

# Glimpse the data
glimpse(ratsass2)

str(ratsass2)

ratsass3 <- filter(ratsass2,mean<550)

str(ratsass3)

ggplot(ratsass3, aes(x = Group, y = mean)) +
  geom_boxplot() +
  stat_summary(fun.y = "mean", geom = "point", shape=23, size=4, fill = "white") +
  scale_y_continuous(name = "mean(Weight), exluding initial Weight")



```

Next we check whether these differences appear to be statistically significant. As there are three groups, ANOVA is used instead of a two-sample t-test. The null hypothesis is that the means of the groups are the same and we test whether they differ significantly both for the average weight of the inspection period (t>WD1) and for the baseline weigh (t=WD1). 

Assumptions for Anova are that the observations are independent from each other, the data of each group is normally distributed and they have a common variance. 

According to the Anova, the null hypothesis can be only rejected with the starting weight. Only the baseline weight is significantly different, otherwise, whatever is fed to those poor rats, is not working, which might be a good thing.

```{r} 

RATS <- read.table("https://raw.githubusercontent.com/KimmoVehkalahti/MABS/master/Examples/data/rats.txt")

ratsass4 <- ratsass2 %>% mutate(baseline = RATS$WD1)

fit <- lm(mean ~ baseline + Group, data = ratsass4)

anova(fit)


```



###II: Implementing the analyses of chapter 9 using the BPRS data. 


The data used in this second part:

*Bprs - 40 males are divided into treatment and control groups with 20 males in each group (By the way, I'm calling treatment=2 the treatment group and treatment=1 the control group). The brief psychiatric rating scores (bprs) of these males are measured weekly  for a total of 8 weeks. Bprs is a rating scale, which measures different psychiatric symptoms ([Wikipedia](https://en.wikipedia.org/wiki/Brief_Psychiatric_Rating_Scale)). The data is originally from Davis (2002).

A few words about  multilevel modeling, if I may. A basic way to distinguish between different multilevel models is to categorize them into i) random intercept model, ii) random slope model and iii) a combination of the two, random intercept + slope model.
Personally, I prefer different naming conventions with "varying intercept model" and "varying slope model", as they are more intuitive, but that might just be me.

What does random or varying mean in this context? Traditionally  with these models, we can take into account that the data structure is nested (*to my understanding that is*). One classic example of a nested data structure could be students from different classes and schools. A usual assumption for regression model is that the observations are independent, but this might not be a valid assumption in such a case. With multilevel models we can allow heterogeineity between groups' (or individuals') slopes and/or intercepts. For instance, these different classes and schools might affect the outcome variable of the students, GPA for instance, in some way. The individual outcomes are correlated by the schools or classes and with multilevel models we can get this sort of main effect and then consequently analyze what kind of effects different nested structures might have on the intercepts or slopes used in the analysis.

It must be noted that here we use multilevel models to allow slopes and intercepts to differ by individual rat. Again, as more traditional regressions assume that the observations are independent, which is a rather invalid assumption with a dependent variable like bprs : bprs at time=0 and bprs at time=1 are unlikely to be independent observations. As such, we take the intraclass correlation of an individual male's bprs score between different time points into account, making this a more robust form of analysis.

On with the show! First I plot the bprs by the treatment and the control groups and it's quite a mess to be honest.


```{r}
bprsl$treatment <- factor(bprsl$treatment)


ggplot(bprsl, aes(x = week, y = bprs, linetype = as.factor(subject))) +
  geom_line() +
  scale_linetype_manual(values = rep(1:10, times=4)) +
  facet_grid(. ~ treatment, labeller = label_both) +
  theme(legend.position = "none") + 
  scale_y_continuous(limits = c(min(bprsl$bprs), max(bprsl$bprs)))

```


Next, a **standard issue linear regression** is performed. The treatment is not  statistically significant, while both groups have a declining trend with time. It must be noted that this regression suffers from the issues mentioned above and ignores the repeated measures structure of the data, which means that any correlation between an individuals observations is not taken into account. A plausible reason why the individual observations might be correlated in time, could be due to an individual's chemical balance of the brain, or whatever unobservables correlated in time there might be.

```{r}
bprsl_reg <- lm(bprs ~ week + treatment, data = bprsl)
summary(bprsl_reg)

```


**Random intercept model** to the rescue! This means, that we allow the intercept to vary for each individual male. So no more strict independence assumed for the observations. But this is likely not enough, as it is easy to imagine the slopes being different across individuals as well.

The estimated variance of the individual random effects is quite large, signaling variation in the fitted individual intercepts.Estimated parameters for treatment and week are similar as in the linear regression estimated above, except for the standard errors, which are a bit smaller. So controlling for the within individual correlation does seem to pay off!

```{r}
bprsl_int <- lmer(bprs ~ week + treatment + (1 | subject), data = bprsl, REML = FALSE)
summary(bprsl_int)

```


**Random intercept and random slope model**

As stated by the name of the model, now each individual is allowed to differ both in terms of intercept and slope. So heterogeineity all over the place is tolerated.

We compare the random intercept and random intercept + slope models with an anova table. Chi-squared and p-value suggests that the random intercept + slope model prevails over just random intercept model (as Datacamp suggests, the lower the values, the better the fit against the comparison model).



```{r}
bprsl_intslope <- lmer(bprs ~ week + treatment + (week | subject), data = bprsl, REML = FALSE)
summary(bprsl_intslope)

anova(bprsl_intslope, bprsl_int)


```


**Random intercept and random slope model with timeXgroup interaction**

The interaction is not statistically significant on a 95% CI, although, some general remarks can be made. Overall, for both groups the BPRS scores seem to go down, but for males in the treatment group, the pace (or slope) is smaller. Something might be going on there, but this would probably need more individuals and more observations for statistical power. 

Or the results could just be random walk of sorts: [Replication Crisis.](https://en.wikipedia.org/wiki/Replication_crisis)

```{r}
bprsl_intslopeX <- lmer(bprs ~ week + treatment + week*treatment + (week | subject), data = bprsl, REML = FALSE)

Fitted <- fitted(bprsl_intslopeX)
bprsl <- bprsl %>% mutate(Fitted)

summary(bprsl_intslopeX)



```


**Comparison: Observed vs fitted bprs by groups** for the ending! Every male has their own slope and intercept. Seems cool! 

**Thanks for reading this far! Have a great remainder of the year and if things get stressful and hectic, remember the capybaras!**


```{r}

bprsl <- mutate(bprsl, subject2 = ifelse(bprsl$treatment==2, bprsl$subject+20, bprsl$subject)) 


ggplot(bprsl, aes(x = week, y = bprs, group = subject2)) + 
  geom_line(aes(linetype = treatment))  +
  geom_line(aes(linetype = treatment)) +  
  scale_linetype_manual(values = rep(1:10, times=4)) +
  facet_grid(. ~ treatment, labeller = label_both) +
  scale_x_continuous(name = "Time (weeks)", breaks = seq(0, 8, 1)) +
  scale_y_continuous(name = "BPRS") +
  theme(legend.position = "top")
  

ggplot(bprsl, aes(x = week, y = Fitted, group = subject2)) +
  geom_line(aes(linetype = treatment)) +  
  scale_linetype_manual(values = rep(1:10, times=4)) +
  facet_grid(. ~ treatment, labeller = label_both) +
  scale_x_continuous(name = "Time (weeks)", breaks = seq(0, 8, 1)) +
  scale_y_continuous(name = "Fitted BPRS") +
  theme(legend.position = "top")





```



