#Chapter 6: Analysis of longitudinal data

**Part 0: Libraries used and overall setup**

```{r echo=TRUE, warning=FALSE} 

rm(list=ls())

library(dplyr); library(ggplot2); library(GGally); library(readr); library(tidyr); library(lme4) 

setwd("~/GitHub/IODS-project/data")

ratsl <- read.table("RATSL.txt")
bprsl <- read.table("BPRSL.txt")

ratsl$ID <- factor(ratsl$ID)
ratsl$Group <- factor(ratsl$Group)

```


###I: Implementing the analyses of chapter 8 using the RATS data 

I am quite slavishly following the chapters in the books.Like a rat following the pied piper. 


The data used in this first part:

*Rats - Three different groups of rats (a total of 16 rats) are fed different food and change in their weight is observed. The data is originally from Crowder & Hand (1990). Used in this first part.


First, I plot the individual rat observations by time. Group signals the colour. 

[*Important notice:* In the Vehkalahti & Everett (2018?) chapter 8, there layout was three graphs, one graph per group that is, but I chose to plot all the rats in the same graph and just distinguish the groups by color, as there are only 16 rats. The information provided by both approaches is somewhat equal.]

Whoa, there is one huge rat in the second group! Otherwise, the initial levels of the rat weight and the slopes seem to be quite close together group-wise.


![](C:\Users\Juha\Desktop\capybara.jpg) The biggest rodent in the world is capybara.

![](C:\Users\Juha\Desktop\capybara.gif) Capybaras are wicked cool animals.

![](C:\Users\Juha\Desktop\capybara2.gif) Respected by all.

[What's not to love.](https://www.smithsonianmag.com/smart-news/capybaras-are-basically-natures-chairs-180949677/)


```{r} 

ggplot(ratsl, aes(x = Time, y = Weight, group=ID, col = Group)) +
  geom_line() +
  scale_linetype_manual(values = rep(1:10, times=4))  + 
  scale_y_continuous(name = "rat weight", limits = c(min(ratsl$Weight), max(ratsl$Weight))) 

```

Now if we standardize the weights (below), the slopes get more equal.There *seems* to be no differences in the growth rates of the rates time-wise. 
 

```{r} 
ratsl <- ratsl %>%
  group_by(Time) %>%
  mutate(stdweight = scale(Weight)) %>%
  ungroup()

ggplot(ratsl, aes(x = Time, y = stdweight, group=ID, col = Group )) +
  geom_line() +
  scale_linetype_manual(values = rep(1:10, times=4)) +
  scale_y_continuous(name = "Standardized rat weight")

```


Next, I plot how the mean weight has changed by group.

Not that much new information here, as anticipated, the standard errors are highest among the second group. Yup, the one with that huge rat.


```{r} 

n <- ratsl$Time %>% unique() %>% length()

ratsass <- ratsl %>%
  group_by(Group, Time) %>%
  summarise( mean = mean(Weight), se = sd(Weight)/sqrt(n)) %>%
  ungroup()

ggplot(ratsass, aes(x = Time, y = mean, linetype = Group, shape = Group)) +
  geom_line() +
  geom_point(size=3) +
  geom_errorbar(aes(ymin=mean-se, ymax=mean+se, linetype="1"), width=0.3) +
  theme(legend.position = c(0.9,0.5)) +
  scale_y_continuous(name = "mean(Weight) +/- se(Weight)")

```



Summary measures approach in the form of boxplots. Boxes by groups. Excluding that one huge rat. The groups differ drastically. 


```{r} 

ratsass2 <- ratsl %>%
  filter(Time > 1) %>%
  group_by(Group, ID) %>%
  summarise( mean=mean(Weight) ) %>%
  ungroup()

# Glimpse the data
glimpse(ratsass2)

str(ratsass2)

ratsass3 <- filter(ratsass2,mean<550)

str(ratsass3)

ggplot(ratsass3, aes(x = Group, y = mean)) +
  geom_boxplot() +
  stat_summary(fun.y = "mean", geom = "point", shape=23, size=4, fill = "white") +
  scale_y_continuous(name = "mean(Weight), exluding initial Weight")



```

Next we check whether these differences appear to be statistically significant. As there are three groups, ANOVA is used instead of a two-sample t-test. The null hypothesis is that the means of the groups are the same and we test whether they differ significantly both for the average weight of the inspection period (t>WD1) and for the baseline weigh (t=WD1). 

Assumptions are, that the observations are independent from each other, the data of each group is normally distributed and they have a common variance. 

According to the Anova, the null hypothesis can be only rejected with the starting weight. Only the baseline weight is significantly different, otherwise, whatever is fed to those poor rats, is not working, which might be a good thing.

```{r} 

RATS <- read.table("https://raw.githubusercontent.com/KimmoVehkalahti/MABS/master/Examples/data/rats.txt")

ratsass4 <- ratsass2 %>% mutate(baseline = RATS$WD1)

fit <- lm(mean ~ baseline + Group, data = ratsass4)

anova(fit)


```



###II: Implementing the analyses of chapter 9 using the BPRS data. 


The data used in this second part:

*Bprs - 40 males are divided into treatment and control groups (20 males in each group). And their brief psychiatric rating scores (bprs) are weekly measured for a total of 8 weeks. Bprs is a rating scale, which measure different psychiatric symptoms ([Wikipedia](https://en.wikipedia.org/wiki/Brief_Psychiatric_Rating_Scale)). The data is originally from Davis (2002).

A few words about  multilevel modeling, if I may. A basic way to distinguish between different multilevel models is to categorize them into i) random intercept model, ii) random slope model and iii) a combination of the two, random intercept + slope model.
Personally, I prefer different naming conventions with "varying intercept model" and "varying slope model", as they are more intuitive, but that might just be me.

What does random or varying mean in this context? Traditionally  with these models, we can take into account that the data structure is nested (*to my understanding that is*). One classic example of a nested data structure could be GPAs of students from different classes and schools. A usual assumption for regression model is that the observations are independent, but this might not be a valid assumption. With multilevel models we can allow heterogeineity between groups or individuals slopes and/or intercepts. For instance, these different classes and schools might affect the outcome variable of the students in some way (their outcomes are correlated by the schools or classes) and with multilevel models we can get this sort of main effect and then consequently analyze what kind of effects different nested structures might have on the intercepts or slopes in the analysis.

It must be noted that here we use multilevel models to allow slopes and intercepts differ by individual rat. The main point is that traditional regressions assume that the observations are independent, which is a rather invalid assumption with a dependent variable like rat weight. Rat weight at time=0 and weight at time=1 are unlikely to be independent observations. As such, we take theintraclass correlation of an individual rat's weight between different time points into account, making this a more robust form of analysis.



ON with the show! First I plot the bprs by the treatment and the control groups. It's quite a mess to be honest. In the plotting of the individual time series, the use of two different graphs for control and treatment groups is justified, even though, it's still kinda messy


```{r}
bprsl$treatment <- factor(bprsl$treatment)


ggplot(bprsl, aes(x = week, y = bprs, linetype = as.factor(subject))) +
  geom_line() +
  scale_linetype_manual(values = rep(1:10, times=4)) +
  facet_grid(. ~ treatment, labeller = label_both) +
  theme(legend.position = "none") + 
  scale_y_continuous(limits = c(min(bprsl$bprs), max(bprsl$bprs)))

```


Then a **standard issue linear regression** is performed. The treatment is not  statistically significant, while both groups have a declining trend with time. It must be noted that this regression suffers from the issues mentioned above and ignores the repated measures structure of the data.

```{r}
bprsl_reg <- lm(bprs ~ week + treatment, data = bprsl)
summary(bprsl_reg)

```


**Random intercept model** to the rescue! This means, that we allow the intercept to vary for each individual male. 


```{r}
bprsl_int <- lmer(bprs ~ week + treatment + (1 | subject), data = bprsl, REML = FALSE)
summary(bprsl_int)

```


**Random intercept and random slope model**


```{r}
bprsl_intslope <- lmer(bprs ~ week + treatment + (week | subject), data = bprsl, REML = FALSE)
summary(bprsl_intslope)

```


**Random intercept and random slope model with timeXgroup interaction**



```{r}
bprsl_intslopeX <- lmer(bprs ~ week + treatment + week*treatment + (week | subject), data = bprsl, REML = FALSE)

Fitted <- fitted(bprsl_intslopeX)
bprsl <- bprsl %>% mutate(Fitted)

summary(bprsl_intslopeX)



```


**Comparison: Observed vs fitted bprs by groups** Here they are! Lol!


```{r}


bprsl <- mutate(bprsl, subject2 = ifelse(bprsl$treatment==2, bprsl$subject+20, bprsl$subject)) 




ggplot(bprsl, aes(x = week, y = bprs, group = subject2)) + 
  geom_line(aes(linetype = treatment))  +
  geom_line(aes(linetype = treatment)) +  
  scale_linetype_manual(values = rep(1:10, times=4)) +
  facet_grid(. ~ treatment, labeller = label_both) +
  scale_x_continuous(name = "Time (weeks)", breaks = seq(0, 8, 1)) +
  scale_y_continuous(name = "BPRS") +
  theme(legend.position = "top")
  

ggplot(bprsl, aes(x = week, y = Fitted, group = subject2)) +
  geom_line(aes(linetype = treatment)) +  
  scale_linetype_manual(values = rep(1:10, times=4)) +
  facet_grid(. ~ treatment, labeller = label_both) +
  scale_x_continuous(name = "Time (weeks)", breaks = seq(0, 8, 1)) +
  scale_y_continuous(name = "Fitted BPRS") +
  theme(legend.position = "top")





```



